{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Enid Blyton Text Generation.ipynb","provenance":[],"authorship_tag":"ABX9TyN2gAgSwk10N7R0SZVF5YDZ"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pIPT2hgZ4d_c","executionInfo":{"elapsed":414,"status":"ok","timestamp":1611791156854,"user":{"displayName":"arjun reddy","photoUrl":"","userId":"09543624328287564749"},"user_tz":480},"outputId":"8625753c-83ff-4415-c320-2e50537a6252"},"source":["import numpy\n","import sys\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, LSTM\n","from keras.utils import np_utils\n","from keras.callbacks import ModelCheckpoint\n","import nltk\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yb_pW2re5k2M","executionInfo":{"status":"ok","timestamp":1611975603761,"user_tz":480,"elapsed":24242,"user":{"displayName":"arjun reddy","photoUrl":"","userId":"09543624328287564749"}},"outputId":"7a3c0e0b-0ce5-4007-fe20-5276f653b4d5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7DR6CXqR7ic0"},"source":["file = open('/content/drive/My Drive/Datasets/FamousFiveCompleteVolume.txt').read()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRU-Dn046Flw"},"source":["def tokenize_words(input):\n","    # lowercase everything to standardize it\n","    input = input.lower()\n","\n","    # instantiate the tokenizer\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    tokens = tokenizer.tokenize(input)\n","\n","    # if the created token isn't in the stop words, make it part of \"filtered\"\n","    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n","    return \" \".join(filtered)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"W1nTlqu19jVW"},"source":["# preprocess the input data, make tokens\n","processed_inputs = tokenize_words(file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"d2Zt9Sk99kwZ"},"source":["chars = sorted(list(set(processed_inputs)))\n","char_to_num = dict((c, i) for i, c in enumerate(chars))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"debJCf7--mko","outputId":"d8a230df-c742-4331-e671-d6e76fa5bed9"},"source":["input_len = len(processed_inputs)\n","vocab_len = len(chars)\n","print (\"Total number of characters:\", input_len)\n","print (\"Total vocab:\", vocab_len)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total number of characters: 2676877\n","Total vocab: 37\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"uTN_zUEt-oUg"},"source":["seq_length = 100\n","x_data = []\n","y_data = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"dMcr8gud-r8N"},"source":["# loop through inputs, start at the beginning and go until we hit\n","# the final character we can create a sequence out of\n","for i in range(0, input_len - seq_length, 1):\n","    # Define input and output sequences\n","    # Input is the current character plus desired sequence length\n","    in_seq = processed_inputs[i:i + seq_length]\n","\n","    # Out sequence is the initial character plus total sequence length\n","    out_seq = processed_inputs[i + seq_length]\n","\n","    # We now convert list of characters to integers based on\n","    # previously and add the values to our lists\n","    x_data.append([char_to_num[char] for char in in_seq])\n","    y_data.append(char_to_num[out_seq])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"qBp8gknj-vGW","outputId":"c5efde48-dea6-4855-eeaf-dbcc2993a491"},"source":["n_patterns = len(x_data)\n","print (\"Total Patterns:\", n_patterns)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Patterns: 2676777\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"_mgXjsyC-zJY"},"source":["X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n","X = X/float(vocab_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"3--zHb_F-01F"},"source":["y = np_utils.to_categorical(y_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"ntnBc-u6-2bp"},"source":["model = Sequential()\n","model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(256, return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(128))\n","model.add(Dropout(0.2))\n","model.add(Dense(y.shape[1], activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"z0_lJEG7_CBc"},"source":["model.compile(loss='categorical_crossentropy', optimizer='adam')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"yuBmeBV6_DZw"},"source":["filepath = \"model_weights_saved.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","desired_callbacks = [checkpoint]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"qokzVwG0_F5A","outputId":"32a32b44-3139-4ea4-a8e8-4efc8fa3fea1"},"source":["model.fit(X, y, epochs=25, batch_size=256, callbacks=desired_callbacks)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/25\n","10457/10457 [==============================] - 794s 75ms/step - loss: 2.3528\n","\n","Epoch 00001: loss improved from inf to 2.05642, saving model to model_weights_saved.hdf5\n","Epoch 2/25\n","10457/10457 [==============================] - 778s 74ms/step - loss: 2.2939\n","\n","Epoch 00002: loss did not improve from 2.05642\n","Epoch 3/25\n","10457/10457 [==============================] - 779s 74ms/step - loss: 2.0438\n","\n","Epoch 00003: loss improved from 2.05642 to 1.95817, saving model to model_weights_saved.hdf5\n","Epoch 4/25\n","10457/10457 [==============================] - 782s 75ms/step - loss: 1.7339\n","\n","Epoch 00004: loss improved from 1.95817 to 1.69227, saving model to model_weights_saved.hdf5\n","Epoch 5/25\n","10457/10457 [==============================] - 781s 75ms/step - loss: 1.5969\n","\n","Epoch 00005: loss improved from 1.69227 to 1.57652, saving model to model_weights_saved.hdf5\n","Epoch 6/25\n","10457/10457 [==============================] - 783s 75ms/step - loss: 1.5254\n","\n","Epoch 00006: loss improved from 1.57652 to 1.51530, saving model to model_weights_saved.hdf5\n","Epoch 7/25\n","10457/10457 [==============================] - 782s 75ms/step - loss: 1.4862\n","\n","Epoch 00007: loss improved from 1.51530 to 1.47834, saving model to model_weights_saved.hdf5\n","Epoch 8/25\n","10457/10457 [==============================] - 787s 75ms/step - loss: 1.4551\n","\n","Epoch 00008: loss improved from 1.47834 to 1.45116, saving model to model_weights_saved.hdf5\n","Epoch 9/25\n","10457/10457 [==============================] - 790s 76ms/step - loss: 1.4364\n","\n","Epoch 00009: loss improved from 1.45116 to 1.43232, saving model to model_weights_saved.hdf5\n","Epoch 10/25\n","10457/10457 [==============================] - 789s 75ms/step - loss: 1.4195\n","\n","Epoch 00010: loss improved from 1.43232 to 1.41604, saving model to model_weights_saved.hdf5\n","Epoch 11/25\n","10457/10457 [==============================] - 790s 76ms/step - loss: 1.4048\n","\n","Epoch 00011: loss improved from 1.41604 to 1.40380, saving model to model_weights_saved.hdf5\n","Epoch 12/25\n","10457/10457 [==============================] - 789s 75ms/step - loss: 1.3946\n","\n","Epoch 00012: loss improved from 1.40380 to 1.39330, saving model to model_weights_saved.hdf5\n","Epoch 13/25\n","10457/10457 [==============================] - 785s 75ms/step - loss: 1.3867\n","\n","Epoch 00013: loss improved from 1.39330 to 1.38475, saving model to model_weights_saved.hdf5\n","Epoch 14/25\n","10457/10457 [==============================] - 785s 75ms/step - loss: 1.3791\n","\n","Epoch 00014: loss improved from 1.38475 to 1.37728, saving model to model_weights_saved.hdf5\n","Epoch 15/25\n","10457/10457 [==============================] - 781s 75ms/step - loss: 1.3717\n","\n","Epoch 00015: loss improved from 1.37728 to 1.37105, saving model to model_weights_saved.hdf5\n","Epoch 16/25\n","10457/10457 [==============================] - 781s 75ms/step - loss: 1.3657\n","\n","Epoch 00016: loss improved from 1.37105 to 1.36566, saving model to model_weights_saved.hdf5\n","Epoch 17/25\n","10457/10457 [==============================] - 783s 75ms/step - loss: 1.3606\n","\n","Epoch 00017: loss improved from 1.36566 to 1.35996, saving model to model_weights_saved.hdf5\n","Epoch 18/25\n","10457/10457 [==============================] - 785s 75ms/step - loss: 1.3566\n","\n","Epoch 00018: loss improved from 1.35996 to 1.35578, saving model to model_weights_saved.hdf5\n","Epoch 19/25\n","10457/10457 [==============================] - 789s 75ms/step - loss: 1.3518\n","\n","Epoch 00019: loss improved from 1.35578 to 1.35146, saving model to model_weights_saved.hdf5\n","Epoch 20/25\n","10457/10457 [==============================] - 788s 75ms/step - loss: 1.3474\n","\n","Epoch 00020: loss improved from 1.35146 to 1.34709, saving model to model_weights_saved.hdf5\n","Epoch 21/25\n","10457/10457 [==============================] - 788s 75ms/step - loss: 1.3441\n","\n","Epoch 00021: loss improved from 1.34709 to 1.34401, saving model to model_weights_saved.hdf5\n","Epoch 22/25\n","10457/10457 [==============================] - 788s 75ms/step - loss: 1.3397\n","\n","Epoch 00022: loss improved from 1.34401 to 1.34079, saving model to model_weights_saved.hdf5\n","Epoch 23/25\n","10457/10457 [==============================] - 788s 75ms/step - loss: 1.3369\n","\n","Epoch 00023: loss improved from 1.34079 to 1.33765, saving model to model_weights_saved.hdf5\n","Epoch 24/25\n","10457/10457 [==============================] - 789s 75ms/step - loss: 1.3337\n","\n","Epoch 00024: loss improved from 1.33765 to 1.33444, saving model to model_weights_saved.hdf5\n","Epoch 25/25\n","10457/10457 [==============================] - 789s 75ms/step - loss: 1.3313\n","\n","Epoch 00025: loss improved from 1.33444 to 1.33194, saving model to model_weights_saved.hdf5\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f98327c78d0>"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"PoMDpcfv_Hwu"},"source":["filename = \"model_weights_saved.hdf5\"\n","model.load_weights(filename)\n","model.compile(loss='categorical_crossentropy', optimizer='adam')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"BJwElsG3wtt-"},"source":["num_to_char = dict((i, c) for i, c in enumerate(chars))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"TcefDm8GwuiZ","outputId":"0ded5bb1-5810-4dfc-faa4-e976a6ee4a6a"},"source":["start = numpy.random.randint(0, len(x_data) - 1)\n","pattern = x_data[start]\n","print(\"Random Seed:\")\n","print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Random Seed:\n","\" hisper excited dog want give hiding place away idiot edgar panting puffing arrived cliff top complet \"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"viV9woyIwwcU","outputId":"3a8508c3-1ad3-4c54-ed63-30e4b1dff7db"},"source":["for i in range(1000):\n","    x = numpy.reshape(pattern, (1, len(pattern), 1))\n","    x = x / float(vocab_len)\n","    prediction = model.predict(x, verbose=0)\n","    index = numpy.argmax(prediction)\n","    result = num_to_char[index]\n","    seq_in = [num_to_char[value] for value in pattern]\n","\n","    sys.stdout.write(result)\n","\n","    pattern.append(index)\n","    pattern = pattern[1:len(pattern)]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ely said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian said julian "],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CD-C0FZ9w4vg"},"source":[""],"execution_count":null,"outputs":[]}]}